{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd98331-65b2-4ed6-ad01-facf44e75557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6b38a1-f951-4770-83ca-3a02acf0c175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get preprocessed train and test data(plus several necessary alterations)\n",
    "ais_train = pd.read_csv(\"xgboost_preprocessed_data/xgboost_data_new100.csv\")\n",
    "cols_to_imp = ['speed_std', 'dist_std', 'x_std', 'y_std'] # Columns to impute with mean\n",
    "ais_train[cols_to_imp] = ais_train[cols_to_imp].fillna(ais_train[cols_to_imp].mean())\n",
    "ais_test = pd.read_csv(\"xgboost_preprocessed_data/xgboost_data_newtest2.csv\")\n",
    "ais_test[cols_to_imp] = ais_test[cols_to_imp].fillna(ais_test[cols_to_imp].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f25bb6-33a4-4272-85ab-b63e89ff0dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ais_train['net'] = (ais_train['red_flags'] >= 3) & (ais_train['net_name'])\n",
    "ais_train['ship'] = ais_train['red_flags'] == 0\n",
    "ais_test['net'] = (ais_test['red_flags'] >= 3) & (ais_test['net_name'])\n",
    "ais_test['ship'] = ais_test['red_flags'] == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0226a6fd-fb84-49b5-804a-9d94aca98bbb",
   "metadata": {},
   "source": [
    "# Make Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d717ef3e-fef6-4c95-879e-b7b8cdfdc559",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ann1(nn.Module):\n",
    "    # Implements a feed-forward neural net\n",
    "    def __init__(self, train_x, train_y, hidden_nodes, batch_size = 32, seed = 12345):\n",
    "        # SUPER INIT\n",
    "        super().__init__()\n",
    "        self.optimizer = None\n",
    "        self.loss_fn = None\n",
    "        # Set seed for reproducibility\n",
    "        torch.manual_seed(seed)\n",
    "        # Make epoch = 0\n",
    "        self.epoch = 0\n",
    "        self.optimal_epochs = np.NaN\n",
    "        # Save batch size\n",
    "        self.batch_size = batch_size\n",
    "            \n",
    "        # Upsample the data before modeling\n",
    "        if train_y[train_y == 1].shape[0] <= train_y[train_y == 0].shape[0]/2:\n",
    "            sampling_strategy = {0: train_y[train_y == 0].shape[0], 1: round(train_y[train_y == 0].shape[0]/2)}\n",
    "            smote = SMOTE(sampling_strategy = sampling_strategy, random_state = 836320)\n",
    "            train_x, train_y = smote.fit_resample(train_x, train_y)            \n",
    "        \n",
    "        # Format data and dependencies\n",
    "        train_x = torch.tensor(train_x, dtype = torch.float32)\n",
    "        train_y = torch.tensor(train_y, dtype = torch.float32)\n",
    "        train_dataset = TensorDataset(train_x, train_y)\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "        self.input_dim0, self.input_dim1 = train_x.shape\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        # Net layers\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(self.input_dim1, hidden_nodes), \n",
    "            nn.PReLU(), \n",
    "            nn.Linear(hidden_nodes, 1), \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x): # Makes the forward pass in the net\n",
    "        ''' \n",
    "        A method to make neural net predictions\n",
    "        '''\n",
    "        self.input_dim0, self.input_dim1 = x.shape\n",
    "        if type(x) == np.ndarray:\n",
    "            x = torch.tensor(x, dtype = torch.float32)\n",
    "        outputs = self.layers(x)\n",
    "        return outputs\n",
    "    \n",
    "    def train_net(self, val_x, val_y, num_epochs, filepath, loss_fn = nn.BCELoss(), learning_rate = .03, report_freq = 1, print_results = True, early_stopping_rounds = 5):\n",
    "        ''' \n",
    "        A method to train the neural net on data\n",
    "        '''\n",
    "        self.loss_fn = loss_fn\n",
    "        val_x = torch.tensor(val_x, dtype = torch.float32)\n",
    "        val_y = torch.tensor(val_y, dtype = torch.float32)\n",
    "        val_dataset = TensorDataset(val_x, val_y)\n",
    "        self.val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        self.train()  # Set the model to training mode\n",
    "\n",
    "        # Define optimizer, number of data points\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr = learning_rate)\n",
    "        dataset_size = self.input_dim0\n",
    "        n_test = val_x.shape[0]\n",
    "        # Make lists for later\n",
    "        train_loss_list = []\n",
    "        train_acc_list = []\n",
    "        val_loss_list = []\n",
    "        val_acc_list = []\n",
    "        for epoch in range(self.epoch, self.epoch + num_epochs):\n",
    "            for x_batch, y_batch in self.train_loader:\n",
    "                # Forward pass \n",
    "                outputs = self.forward(x_batch)\n",
    "                # Compute loss \n",
    "                loss = loss_fn(outputs, y_batch.unsqueeze(1))\n",
    "                # Backpropagation \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            threshold = .5 # Set probability threshold for classification \n",
    "            # Get epoch train results\n",
    "            with torch.no_grad():\n",
    "                outputs = self.forward(self.train_x)\n",
    "                train_loss = loss_fn(outputs, self.train_y.unsqueeze(1))\n",
    "                train_accuracy = torch.sum((outputs >= threshold) == self.train_y.unsqueeze(1))/dataset_size\n",
    "\n",
    "            train_loss_list.append(train_loss.item())\n",
    "            train_acc_list.append(train_accuracy.item())\n",
    "            # Get epoch's validation set results\n",
    "            with torch.no_grad():\n",
    "                val_preds = self.forward(val_x)\n",
    "                val_loss = loss_fn(val_preds, val_y.unsqueeze(1))\n",
    "                val_accuracy = torch.sum((val_preds >= threshold) == val_y.unsqueeze(1))/n_test\n",
    "\n",
    "            val_loss_list.append(val_loss.item())\n",
    "            val_acc_list.append(val_accuracy.item())\n",
    "            # Print epoch results?\n",
    "            if print_results:\n",
    "                if (epoch + 1) % report_freq == 0 or epoch == 0 or (epoch + 1) == num_epochs:\n",
    "                    print(f\"Epoch {epoch+1}/{self.epoch + num_epochs}, Train Loss: {train_loss.item():.4f}, Train Accuracy: {train_accuracy: .4f}, \\\n",
    "                    Val Loss: {val_loss.item(): .4f}, Val Accuracy: {val_accuracy: .4f}\")\n",
    "            # Save model if it is the best so far\n",
    "            if epoch == 0 or min(val_loss_list) == val_loss:\n",
    "                checkpoint = {\n",
    "                    'model_state_dict': self.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'train_loss_list': train_loss_list,\n",
    "                    'train_acc_list': train_acc_list,\n",
    "                    'val_loss_list': val_loss_list,\n",
    "                    'val_acc_list': val_acc_list\n",
    "                }\n",
    "                torch.save(checkpoint, filepath)\n",
    "            # Add early stopping \n",
    "            elif val_loss_list.index(min(val_loss_list)) == epoch - early_stopping_rounds:\n",
    "                break\n",
    "        # Make epochs \n",
    "        self.epoch = self.epoch + num_epochs\n",
    "        # Print best number of epochs \n",
    "        optimal_epochs = [ind + 1 for ind, val in enumerate(val_loss_list) if val == min(val_loss_list)][0]\n",
    "        self.optimal_epochs = optimal_epochs\n",
    "        if print_results:\n",
    "            print(f\"The optimal number of epochs was: {optimal_epochs}\")\n",
    "        self.load_best_model(filename = filepath)\n",
    "    \n",
    "    def evaluate(self, test_x, test_y, loss_fn = nn.BCELoss()):\n",
    "        test_x = torch.tensor(test_x, dtype = torch.float32)\n",
    "        test_y = torch.tensor(test_y, dtype = torch.float32)\n",
    "        with torch.no_grad():\n",
    "            test_preds = self.forward(test_x)\n",
    "            test_accuracy = torch.sum((test_preds >= .5) == test_y.unsqueeze(1))/test_x.shape[0]\n",
    "            test_sensitivity = torch.sum((test_preds >= .5) & (test_y.unsqueeze(1) == True))/(torch.sum((test_preds >= .5) & (test_y.unsqueeze(1) == True)) + torch.sum((test_preds <= .5) & (test_y.unsqueeze(1) == True)))\n",
    "            test_specificity = torch.sum((test_preds <= .5) & (test_y.unsqueeze(1) == False))/(torch.sum((test_preds <= .5) & (test_y.unsqueeze(1) == False)) + torch.sum((test_preds >= .5) & (test_y.unsqueeze(1) == False)))\n",
    "            test_loss = loss_fn(test_preds, test_y.unsqueeze(1))\n",
    "        print(f\"The test accuracy is:{test_accuracy: .4f}, test loss is:{test_loss: .4f}, test sensitivity is:{test_sensitivity: .4f}, test specificity is:{test_specificity: .4f}\")\n",
    "    \n",
    "    def predict(self, new_x):\n",
    "        '''\n",
    "        Predict probabilities of a selected dataset\n",
    "        '''\n",
    "        new_x = torch.tensor(new_x, dtype = torch.float32)\n",
    "        with torch.no_grad():\n",
    "            preds = self.forward(new_x)\n",
    "        return preds\n",
    "    \n",
    "    def load_best_model(self, filename):\n",
    "        checkpoint = torch.load(filename)\n",
    "        model_dict = self.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in checkpoint['model_state_dict'].items() if k in model_dict}\n",
    "        self.load_state_dict(pretrained_dict)\n",
    "        if self.optimizer is None:\n",
    "            self.optimizer = optim.Adam(self.parameters(), lr = .03)\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.epoch = checkpoint['epoch']\n",
    "        self.train_loss_list = checkpoint['train_loss_list']\n",
    "        self.train_acc_list = checkpoint['train_acc_list']\n",
    "        self.val_loss_list = checkpoint['val_loss_list']\n",
    "        self.val_acc_list = checkpoint['val_acc_list']            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7adf82-ed3e-4dae-8a1f-07beaed67719",
   "metadata": {},
   "source": [
    "# Get data ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f46734f-f714-498e-bf13-417651af4c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the ones we think are def nets and def not nets\n",
    "max_red_flags = 4\n",
    "prob_nets = ais_train.query(\"red_flags >= 3 & net_name\").copy().reset_index(drop=True)\n",
    "prob_nets['net'] = 1\n",
    "prob_ships = ais_train.query(\"red_flags == 0\").copy().reset_index(drop=True)\n",
    "prob_ships['net'] = 0\n",
    "# Combine the datasets of probable good/bad\n",
    "model_data = pd.concat([prob_nets, prob_ships])\n",
    "x_mat = model_data.loc[:, ['speed_0', 'speed_med', 'speed_99', 'speed_std', 'dist_med', 'dist_99', 'dist_std', \n",
    "                           'x_0', 'x_med', 'x_99', 'x_std', 'y_0', 'y_med', 'y_99', 'y_std']].values\n",
    "y_mat = model_data['net'].values\n",
    "# Split data\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_mat, y_mat, test_size = 0.15, random_state=369)\n",
    "del x_mat, y_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d91c677-f6c7-40af-98ec-6655060905ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for test data\n",
    "prob_nets = ais_test.query(\"red_flags >= 3 & net_name\").copy().reset_index(drop=True)\n",
    "prob_nets['net'] = 1\n",
    "prob_ships = ais_test.query(\"red_flags == 0\").copy().reset_index(drop=True)\n",
    "prob_ships['net'] = 0\n",
    "# Combine the datasets of probable good/bad\n",
    "model_test = pd.concat([prob_nets, prob_ships])\n",
    "x_test_model = model_test.loc[:, ['speed_0', 'speed_med', 'speed_99', 'speed_std', 'dist_med', 'dist_99', \n",
    "                                  'dist_std', 'x_0', 'x_med', 'x_99', 'x_std', 'y_0', 'y_med', 'y_99', 'y_std']].values\n",
    "y_test_model = model_test['net'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1319a3d3-126b-4a44-80df-919bf5b3324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all test data\n",
    "x_test = ais_test.loc[:, ['speed_0', 'speed_med', 'speed_99', 'speed_std', 'dist_med', 'dist_99', 'dist_std', 'x_0', \n",
    "                                   'x_med', 'x_99', 'x_std', 'y_0', 'y_med', 'y_99', 'y_std']].values\n",
    "flags_test = ais_test.loc[:, ['red_flags']].values\n",
    "names_test = ais_test.loc[:, ['net_name']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292819ca-ab47-4e12-badc-45031c0917b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format unlabeled data for semi-supervised learning\n",
    "x_unlabeled = ais_train.query(\"(red_flags != 0) & ((net_name == False) | ((net_name == True) & (red_flags < 3)))\").copy().reset_index(drop=True)\n",
    "x_unlabeled = x_unlabeled.loc[:, ['speed_0', 'speed_med', 'speed_99', 'speed_std', 'dist_med', 'dist_99', 'dist_std', \n",
    "                                  'x_0', 'x_med', 'x_99', 'x_std', 'y_0', 'y_med', 'y_99', 'y_std']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84acf8e-2686-4515-9fb9-61d71c7fdd64",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc522876-b74e-45dd-9162-c92b05077019",
   "metadata": {},
   "source": [
    "### Semi-supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaaaf14-999c-4b04-b26e-bf4b9b6321cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rounds = 5\n",
    "semi_x = x_train\n",
    "semi_y = y_train\n",
    "semi_unlabeled = x_unlabeled\n",
    "unassigned = semi_unlabeled.shape[0]\n",
    "print(f'''Pre-modeling: {sum(semi_y == 1)} probable nets and {sum(semi_y == 0)} probable ships with {semi_unlabeled.shape[0]} out of model.''')\n",
    "for num_round in range(1, num_rounds + 1):\n",
    "    # Train model\n",
    "    model1 = ann1(semi_x, semi_y, hidden_nodes = 20, seed = 2345)\n",
    "    model1.train_net(val_x = x_val, val_y = y_val, num_epochs = 100, learning_rate = .005, report_freq = 1, filepath = \"models/ann1.pth\", early_stopping_rounds = 4)\n",
    "    # Reassign unlabeled data\n",
    "    preds = model1.predict(semi_unlabeled)\n",
    "    new_nets = semi_unlabeled[(preds >= .985).squeeze().numpy()]\n",
    "    semi_x = np.concatenate((semi_x, new_nets))\n",
    "    semi_y = np.concatenate((semi_y, np.ones(new_nets.shape[0])))\n",
    "    new_ships = semi_unlabeled[(preds <= .015).squeeze().numpy()]\n",
    "    semi_x = np.concatenate((semi_x, new_ships))\n",
    "    semi_y = np.concatenate((semi_y, np.zeros(new_ships.shape[0])))\n",
    "    semi_unlabeled = semi_unlabeled[((preds >= .015) & (preds <= .985)).squeeze().numpy()]\n",
    "    print(f'''After round {num_round}: {sum(semi_y == 1)} probable nets and {sum(semi_y == 0)} probable ships with {semi_unlabeled.shape[0]} out of model.''')\n",
    "    if ((semi_unlabeled.shape[0] == 0) | (semi_unlabeled.shape[0] == unassigned)):\n",
    "        break\n",
    "    unassigned = semi_unlabeled.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb85bca-9494-483e-9c07-e05587e42d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model1.predict(x_test) \n",
    "test_preds = pd.concat([pd.DataFrame(flags_test, columns = [\"red_flags\"]), \n",
    "                        pd.DataFrame(test_preds, columns = [\"prob_net\"]),\n",
    "                        pd.DataFrame(names_test, columns = [\"net_name\"])], \n",
    "                        axis = 1)\n",
    "print(\n",
    "    (test_preds[test_preds['prob_net'] >= 0.5].groupby('red_flags').size() / test_preds.groupby('red_flags').size() * 100).fillna(0)\n",
    ")\n",
    "print(\n",
    "    (test_preds[test_preds['prob_net'] >= 0.5].groupby('net_name').size() / test_preds.groupby('net_name').size() * 100).fillna(0)\n",
    ")\n",
    "model1.evaluate(x_test_model, y_test_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fafdf67-ed4f-40ea-94b5-95a018688293",
   "metadata": {},
   "source": [
    "### Not semi-supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8c55ce-9fcb-4809-9515-944f25abcc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model0 = ann1(x_train, y_train, hidden_nodes = 20, seed = 2345)\n",
    "model0.train_net(val_x = x_val, val_y = y_val, num_epochs = 200, learning_rate = .005, report_freq = 1, filepath = \"ann0.pth\", early_stopping_rounds = 5)\n",
    "test_preds = model0.predict(x_test) \n",
    "test_preds = pd.concat([pd.DataFrame(flags_test, columns = [\"red_flags\"]), \n",
    "                        pd.DataFrame(test_preds, columns = [\"prob_net\"]),\n",
    "                        pd.DataFrame(names_test, columns = [\"net_name\"])], \n",
    "                        axis = 1)\n",
    "print(\n",
    "    (test_preds[test_preds['prob_net'] >= 0.5].groupby('red_flags').size() / test_preds.groupby('red_flags').size() * 100).fillna(0)\n",
    ")\n",
    "print(\n",
    "    (test_preds[test_preds['prob_net'] >= 0.5].groupby('net_name').size() / test_preds.groupby('net_name').size() * 100).fillna(0)\n",
    ")\n",
    "model0.evaluate(x_test_model, y_test_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
