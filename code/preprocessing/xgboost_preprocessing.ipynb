{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b15844fa-3885-4b4a-941a-29b980be87a5",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f14ecd9-1ce1-4e8e-ad8a-49828a1276a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "# To check vessel names\n",
    "import re\n",
    "# To get exact lat/long\n",
    "from shapely.wkt import loads\n",
    "# To map the ships or coastlines\n",
    "import geopandas as gpd\n",
    "from geopy.distance import geodesic\n",
    "# To find coastline distance\n",
    "from scipy.spatial import KDTree\n",
    "# For XgBoosting\n",
    "import xgboost as xgb\n",
    "import random\n",
    "# For upsampling during the modeling\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd5b2774-3b38-4542-9330-33c63ef80721",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data formatting \n",
    "this_dir = \"/sfs/gpfs/tardis/project/SDS/capstones/ds6013/iuu_fishing/retry2/east_asia\"\n",
    "files = glob.glob(this_dir + '/*.csv')\n",
    "files.sort()\n",
    "files = files[1000:1100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "772f2b5a-507d-41a9-9ec6-99b722ae45bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set dtypes to avoid errors later\n",
    "dtypes = {'FID': 'object', 'mmsi': 'int64', 'source_id': 'int64', 'imo': 'float64', \n",
    "          'vessel_name': 'object', 'callsign': 'object', 'vessel_type': 'object', \n",
    "          'vessel_type_code': 'float64', 'vessel_type_cargo': 'object', 'vessel_class': 'object',\n",
    "          'length': 'float64', 'width': 'float64', 'flag_country': 'object', 'flag_code': 'float64',\n",
    "          'destination': 'object', 'eta': 'int64', 'draught': 'float64', 'position': 'object',\n",
    "          'latitude': 'float64', 'longitude': 'float64', 'sog': 'float64', 'cog': 'float64',\n",
    "          'rot': 'float64', 'heading': 'int64', 'nav_status': 'object', 'nav_status_code': 'int64',\n",
    "          'source': 'object', 'ts_pos_utc': 'int64', 'ts_static_utc': 'int64', \n",
    "          'dt_pos_utc': 'object', 'dt_static_utc': 'object', 'vessel_type_main': 'float64',\n",
    "          'vessel_type_sub': 'float64', 'message_type': 'int64', 'eeid': 'float64', 'dtg': 'object'}\n",
    "\n",
    "# Read each CSV file into a list of DataFrames\n",
    "dfs = [pd.concat(pd.read_csv(file, dtype = dtypes, chunksize = 100000)) for file in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bec33527-8c9f-4f4e-bcb9-95b3e6d6c0e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Concatenate the DataFrames into a single DataFrame\n",
    "ais = pd.concat(dfs, ignore_index=True).reset_index(drop = True)\n",
    "ais['dtg'] = pd.to_datetime(ais['dtg'])\n",
    "ais = ais.query(\"25 < latitude < 35 & 120 < longitude < 130\")\n",
    "ais = ais.sort_values(['mmsi', 'dtg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3c8602f-ced0-4ec9-82a9-34d9c786a229",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fix NA names\n",
    "ais.loc[ais['vessel_name'].isnull(), 'vessel_name'] = \" \"\n",
    "\n",
    "# Calculate the time difference between consecutive rows for each mmsi\n",
    "ais['time_diff'] = ais.groupby('mmsi')['dtg'].diff()\n",
    "\n",
    "# Create a new column called first_occurrence based on the identified conditions\n",
    "ais['first_occurrence'] = ((ais['time_diff'].isnull()) | (ais['time_diff'] > pd.Timedelta(hours=4))).astype(int)\n",
    "ais['occ_num'] = ais.groupby('mmsi').agg(occ_num = ('first_occurrence', 'cumsum')) # Separate different trips\n",
    "\n",
    "# Get distance traveled\n",
    "ais['old_lat'] = ais.groupby(['mmsi', 'occ_num'])['latitude'].shift(1)\n",
    "ais['old_long'] = ais.groupby(['mmsi', 'occ_num'])['longitude'].shift(1)\n",
    "\n",
    "# Get distance and spped from consecutive readings\n",
    "def calculate_distance(row):\n",
    "    if pd.notna(row['old_lat']) and pd.notna(row['old_long']) and pd.notna(row['latitude']) and pd.notna(row['longitude']):\n",
    "        return geodesic((row['old_lat'], row['old_long']), (row['latitude'], row['longitude'])).nautical\n",
    "    else:\n",
    "        return pd.NA\n",
    "\n",
    "ais['distance'] = ais.apply(calculate_distance, axis = 1)\n",
    "ais['distance'] = pd.to_numeric(ais['distance'], errors='coerce').fillna(0)\n",
    "ais['speed'] = ais['distance'] / (ais['time_diff'].dt.seconds.replace(0, pd.NA) / 3600)\n",
    "ais['distance'] = ais.groupby(['mmsi', 'occ_num']).agg(distance = ('distance', 'cumsum'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68061f2c-b1c9-4dc3-94a3-193116cea731",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Check naming conventions\n",
    "# Look for sketchy names\n",
    "def check_net_name(name):\n",
    "    name = name.lower()\n",
    "    net_name = ('%' in name or \n",
    "                'buoy' in name or \n",
    "                'net' in name or \n",
    "                bool(re.search(r\"\\d+v\", name)))\n",
    "    if net_name is np.nan:\n",
    "        net_name = 0\n",
    "    return net_name\n",
    "\n",
    "ship_names = ais['vessel_name'].astype('str').unique()\n",
    "net_names = [check_net_name(name) for name in ship_names]\n",
    "ais['net_name'] = ais['vessel_name']\\\n",
    "    .map(dict(zip(ship_names, net_names)))\n",
    "ais['net_name'] = ais.groupby(['mmsi', 'occ_num'])['net_name'].transform('max')\n",
    "# Look for bad mmsi values\n",
    "ais['mmsi_length'] = ais['mmsi'].astype('str').str.len() != 9\n",
    "del ship_names, net_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c0b5ec3-c854-4d46-803f-f0392f6d7abd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for starting in water\n",
    "# Read in coastline data from https://www.naturalearthdata.com/downloads/10m-physical-vectors/\n",
    "coastline = pd.concat([gpd.read_file(\"/sfs/gpfs/tardis/project/SDS/capstones/ds6013/iuu_fishing/data/ne_10m_land/ne_10m_land.shp\"), \n",
    "                       gpd.read_file(\"/sfs/gpfs/tardis/project/SDS/capstones/ds6013/iuu_fishing/data/ne_10m_minor_islands/ne_10m_minor_islands.shp\")])\n",
    "coastline = coastline.cx[min(ais['longitude'])-1:max(ais['longitude'])+1, \n",
    "                         min(ais['latitude'])-1:max(ais['latitude'])+1].geometry\n",
    "# Make function to find coastline distance\n",
    "\n",
    "def distance_to_coast(df, coastline):\n",
    "    # Get coordinates of all of the ships\n",
    "    ship_coords = list(df.apply(lambda x: (x.longitude, x.latitude), \n",
    "                                axis = 1))\n",
    "    # Get coordinates of the coastlines\n",
    "    coast_coords = [] \n",
    "    for geom in coastline.geometry:\n",
    "        if geom.geom_type == 'Polygon':\n",
    "            coast_coords.extend(list(geom.exterior.coords))\n",
    "        elif geom.geom_type == 'MultiPolygon':\n",
    "            for polygon in geom.geoms:\n",
    "                coast_coords.extend(list(polygon.exterior.coords))\n",
    "    # Use KDTree to find the nearest points to each ship\n",
    "    tree = KDTree(coast_coords)\n",
    "    idx = tree.query(ship_coords, k=1)[1]\n",
    "    coast_points = pd.Series(coast_coords).iloc[idx]\n",
    "    # Get the distances in nautical miles\n",
    "    naut_dist = [geodesic((ship[1], ship[0]), (coast[1], coast[0])).nautical \n",
    "                 for ship, coast \n",
    "                 in zip(ship_coords, list(coast_points))]\n",
    "    # Insert into the dataframe\n",
    "    df['distance_to_coast'] = naut_dist\n",
    "\n",
    "    return df\n",
    "\n",
    "# Find distance to shore at start\n",
    "first_time = ais['dtg'].min()\n",
    "new_vessels = ais.loc[(ais['first_occurrence'] == 1) & (ais['dtg'] - first_time >= pd.Timedelta(hours = 4)), \n",
    "                      ['mmsi', 'occ_num', 'latitude', 'longitude']]\n",
    "new_vessels = distance_to_coast(new_vessels, coastline)\n",
    "new_vessels['spawn_offshore'] = new_vessels['distance_to_coast'] >= 1\n",
    "\n",
    "# Make dictionary and map to dataframe\n",
    "spawn_dict = {}\n",
    "for index, row in new_vessels.iterrows():\n",
    "    # Combine mmsi and occ_num to create the key\n",
    "    key = (row['mmsi'], row['occ_num'])\n",
    "    # Assign the value of 'spawn_offshore' to the key in the dictionary\n",
    "    spawn_dict[key] = row['spawn_offshore']\n",
    "ais['spawn_offshore'] = ais.apply(lambda row: (row['mmsi'], row['occ_num']), axis=1)\\\n",
    "    .map(spawn_dict)\n",
    "ais['spawn_offshore'] = ais['spawn_offshore'].fillna(False)\n",
    "del index, row, key, coastline, spawn_dict, new_vessels, first_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "226db889-cc2f-4320-ba22-e3914012ae81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If it is clearly spoofing location \n",
    "ais['spoof'] = ais['speed'] >= 150 # World record is 58.1, so allow for some measurement error \n",
    "ais['spoof'] = ais.groupby(['mmsi', 'occ_num'])['spoof'].transform('max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0179d73-4e58-4378-8ad1-a718b76a6b1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add all potential issues together\n",
    "ais['red_flags'] = ais['net_name'].astype('int') + ais['mmsi_length'].astype('int') + \\\n",
    "    ais['spawn_offshore'].astype('int') + ais['spoof'].astype('int')\n",
    "# Transform latitude and longitude to normalized values\n",
    "min_lat, max_lat = ais['latitude'].min(), ais['latitude'].max()\n",
    "min_lon, max_lon = ais['longitude'].min(), ais['longitude'].max()\n",
    "\n",
    "\n",
    "# Normalize latitude and longitude values to the range [0, 1]\n",
    "ais['x'] = (ais['longitude'] - min_lon) / (max_lon - min_lon)\n",
    "ais['y'] = (ais['latitude'] - min_lat) / (max_lat - min_lat)\n",
    "# Get rid of extraneous columns\n",
    "ais = ais.loc[:, ['mmsi', 'occ_num', 'longitude', 'latitude', 'x', 'y', 'sog', 'cog', \n",
    "                  'rot', 'distance', 'dtg', 'net_name', 'mmsi_length', 'spawn_offshore', \n",
    "                  'spoof', 'red_flags']]\n",
    "del min_lat, max_lat, min_lon, max_lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5e62cea-4ddf-48b0-a2da-80a1dd32a091",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prep data for XGBoost\n",
    "ais_model_data = ais.groupby(['mmsi', 'occ_num'])\\\n",
    "    .agg(\n",
    "        net_name = ('net_name', 'max'),\n",
    "        mmsi_length = ('mmsi_length', 'max'),\n",
    "        spawn_offshore = ('spawn_offshore', 'max'),\n",
    "        spoof = ('spoof', 'max'),\n",
    "        speed_0 = ('sog', 'min'),\n",
    "        speed_med = ('sog', 'median'),\n",
    "        speed_99 = ('sog', 'max'),\n",
    "        speed_std = ('sog', 'std'),\n",
    "        dist_med = ('distance', 'median'),\n",
    "        dist_99 = ('distance', 'max'),\n",
    "        dist_std = ('distance', 'std'), \n",
    "        x_0 = ('x', 'min'),\n",
    "        x_med = ('x', 'median'),\n",
    "        x_99 = ('x', 'max'),\n",
    "        x_std = ('x', 'std'),\n",
    "        y_0 = ('y', 'min'),\n",
    "        y_med = ('y', 'median'),\n",
    "        y_99 = ('y', 'max'),\n",
    "        y_std = ('y', 'std'),\n",
    "        red_flags = ('red_flags', 'max'),\n",
    "        entries = ('net_name', 'size')\n",
    ")\n",
    "ais_model_data['dist_med'] = pd.to_numeric(ais_model_data['dist_med'], errors='coerce')\n",
    "ais_model_data['dist_99'] = pd.to_numeric(ais_model_data['dist_99'], errors='coerce')\n",
    "ais_model_data['dist_std'] = pd.to_numeric(ais_model_data['dist_std'], errors='coerce')\n",
    "ais_model_data = ais_model_data.fillna(pd.NA) # Set to pd.NA for future reasons\n",
    "ais_model_data = ais_model_data.reset_index()\n",
    "#add line for write csv -- save xgboost data\n",
    "ais_model_data.to_csv('xgboost_preprocessed_data/xgboost_data_newtest2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda45721-606b-455d-a375-a79a5f208821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b64715-407c-4f2b-9fe8-b78f9d0487be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b7bbe2-9dcf-41ac-9882-d97a52327e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
